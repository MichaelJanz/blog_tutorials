{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, word embeddings are explained in a simple way. Together with python and bpemb we do different kind of experiments to see what actually word embeddings are and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this the bpemb byte-pair encodings by https://github.com/bheinzerling/bpemb\n",
    "These need to be imported first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "import numpy as np\n",
    "# a smaller german model is used, so vocab_size is set to 25000 and dimensions is set to 25\n",
    "bpemb_ger = BPEmb(lang=\"de\", vs=25000, dim=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean, dimensions are set to 25? What are word embeddings at all?\n",
    "\n",
    "Lets have a look at them in a simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words that are about to be inspected are first defined\n",
    "word1 = \"Mann\"\n",
    "word2 = \"Frau\"\n",
    "word3 = \"Tochter\"\n",
    "word4 = \"Kind\"\n",
    "\n",
    "words = [word1, word2, word3]\n",
    "# Now a function _encode is defined, which will create very simple word_embeddings from those words\n",
    "def _encode(words):\n",
    "    # First the words are mapped to indices so it is easier to handle them\n",
    "    word_indices = [i for i in range(len(words))]\n",
    "    \n",
    "    # Then these words should be put in a multi dimensional list, where each dimension has a specific or unspecific meaning. We define 2 Dimensions here:\n",
    "    word_vectors = {i:np.array([0,0]) for i in word_indices}\n",
    "    # Since this is a simple example, we are allowed to define the \"meaning\" of the dimensions by ourselves\n",
    "    # So lets give the first dimension the name grownup, because it should define if someone is grownup\n",
    "    # The seconds dimension gets the name related, because it defines if the context is about beeing related to someone\n",
    "    \n",
    "    # so now we can define our word_vectors manually with those meanings\n",
    "    ## Mann - Man / Husband\n",
    "    word_vectors[0][0] = 1\n",
    "    word_vectors[0][1] = -1\n",
    "    ## Frau - Woman / Wife\n",
    "    word_vectors[1][0] = 1\n",
    "    word_vectors[1][1] = -1\n",
    "    ## Tochter - Daughter\n",
    "    word_vectors[2][0] = 2\n",
    "    word_vectors[2][1] = -2\n",
    "    ## Kind - kid / children\n",
    "    word_vectors[2][0] = 2\n",
    "    word_vectors[2][1] = 0\n",
    "    \n",
    "    # Now the word vectors are defined\n",
    "    \n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([ 1, -1]), 1: array([ 1, -1]), 2: array([2, 0])}\n"
     ]
    }
   ],
   "source": [
    "word_vectors = _encode(words)\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy allows us to do list wise calculations, so we can do mathematic operations on these word_vectors\n",
    "Lets find out what we get, what happens, when \"Mann\" and \"Frau\" are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, \"Mann\" has index 0 and \"Frau\" has index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_word_vector = word_vectors[0] + word_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 -2]\n"
     ]
    }
   ],
   "source": [
    "print(combined_word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combined word vector contains the values of the combination of the words \"Mann\" and \"Frau\" which are the same as the values of the word \"Tochter\". This hard coded model just learned an assiciation, that man and women combined point at daughter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings in bpemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know about the dimension-parameter of the bpemb import. But let some words be said about the vocab_size parameter.\n",
    "Generally, each vocab in NLP should use tokens, that represent rare or unknown word-tokens. Depending on your hardware, the vocab size will restrict you at some point in your model building, since it will run out of memory. Simply said, having a vocab_size of 25.000 word-tokens means having 24.999 real-word tokens and 1 token, that represents the rare ones. (There might also be other tokens, like for special encodings, or other special word parts). Increasing the vocab size to 50.000 will result in more different word_tokens, that, in the 25.000 vs model, would be \"hidden\" in the rare token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bpemb, we have pretrained word vectors, that allow us to do some experiments to understand them better. What happens, when we do the same as above in bpemb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁frau', 0.9576594233512878),\n",
       " ('▁mann', 0.9351305365562439),\n",
       " ('▁freund', 0.8832379579544067),\n",
       " ('▁mutter', 0.8588590621948242),\n",
       " ('▁vater', 0.8556250333786011),\n",
       " ('▁kind', 0.8479531407356262)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Mann')\n",
    "embeds2 = bpemb_ger.embed('Frau')\n",
    "\n",
    "embed_add = embeds1 + embeds2\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop, lets go through each line to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69423   0.142541  0.367741  0.114064 -0.191817  0.329579 -0.181716\n",
      "   1.041099 -1.150414  0.57117  -0.039958  0.936779  0.35633  -0.221654\n",
      "   0.423722  0.433451 -0.308177 -0.793244  0.912121 -0.83934  -0.194813\n",
      "   0.740967 -0.4088    0.140494  0.922742]]\n"
     ]
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Mann')\n",
    "print(embeds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the .embed function of our bpemb model, which tokenizes our input 'Mann' into subtokens, that are used by bpemb and then returns the corresponding word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list, containing lists of 25 elements (as the dimensions, we choose). Since \"Mann\" only results in one token, we only have one list of 25 values.\n",
    "\n",
    "We can add numpy arrays in python, so the lists are merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_add = embeds1 + embeds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have now, is a new vector containing the following Information: We want a word, that has the information of adding \"Mann\" and \"Frau\" together. For this we use .most_similar, with getting the top 6 results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁frau', 0.9576594233512878),\n",
       " ('▁mann', 0.9351305365562439),\n",
       " ('▁freund', 0.8832379579544067),\n",
       " ('▁mutter', 0.8588590621948242),\n",
       " ('▁vater', 0.8556250333786011),\n",
       " ('▁kind', 0.8479531407356262)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"_frau\" and \"_mann\" are still near our start vector, but we also find other words like \"_freund\" (friend) or mother and father. At the 6.th position we find \"_kind\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we teached our predefined _embed method, which information each dimension contains, the bpemb model had to learn it on his own by alot of wikipedia articles. So how about we try to find out more about the meaning of each dimension? For this, we iterate through the dimensions of our embed_add vector and set the to 1 and print the top 3 results. Lets see if we can \"kid\" or even \"daughter\" to the top with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁frau', 0.8940346240997314), ('▁mann', 0.8566429615020752), ('▁freund', 0.808347761631012)]\n",
      "[('▁frau', 0.8886023759841919), ('▁mann', 0.8520528078079224), ('▁geliebte', 0.8106622695922852)]\n",
      "[('▁frau', 0.8825910091400146), ('▁mann', 0.853992223739624), ('▁freund', 0.8089397549629211)]\n",
      "[('▁frau', 0.8841907978057861), ('▁mann', 0.8498778343200684), ('▁freund', 0.8016359806060791)]\n",
      "[('▁frau', 0.8412380814552307), ('▁mann', 0.8276678323745728), ('▁freund', 0.7763669490814209)]\n",
      "[('▁frau', 0.8366867899894714), ('▁mann', 0.8289816975593567), ('▁freund', 0.7783395051956177)]\n",
      "[('▁mann', 0.812994122505188), ('▁frau', 0.7430222630500793), ('▁freund', 0.7360460162162781)]\n",
      "[('▁mann', 0.7824007272720337), ('felt', 0.7378767132759094), ('▁eigentlich', 0.7248014211654663)]\n",
      "[('▁klar', 0.6995150446891785), ('▁gerade', 0.659595787525177), ('▁eigentlich', 0.6595355868339539)]\n",
      "[(',', 0.6670671701431274), ('▁paar', 0.6461077928543091), ('▁klar', 0.6414101123809814)]\n",
      "[('▁klar', 0.6853453516960144), ('▁gerade', 0.6187617778778076), ('▁enttäus', 0.6111329793930054)]\n",
      "[('▁klar', 0.6924160718917847), ('▁gerade', 0.6136806011199951), ('paar', 0.6119207143783569)]\n",
      "[('▁klar', 0.6746844053268433), ('paar', 0.6257284283638), ('▁paar', 0.6127488613128662)]\n",
      "[('▁klar', 0.6750163435935974), ('▁hinter', 0.6560536026954651), ('▁halb', 0.6483876705169678)]\n",
      "[('▁klar', 0.6728219985961914), ('▁hinter', 0.6533928513526917), ('▁halb', 0.6446695327758789)]\n",
      "[('▁klar', 0.6776669025421143), ('▁gerade', 0.6607164740562439), ('▁halb', 0.6388657093048096)]\n",
      "[('▁seite', 0.6523836851119995), ('▁griff', 0.6437400579452515), ('▁klar', 0.6390032172203064)]\n",
      "[('▁wirft', 0.6730402112007141), ('▁enttäus', 0.6729576587677002), ('▁schlägt', 0.6632205247879028)]\n",
      "[('▁enttäus', 0.6794372200965881), ('ähern', 0.6544743776321411), ('▁wirft', 0.64780193567276)]\n",
      "[('▁griff', 0.6711666584014893), ('▁matt', 0.6624730825424194), ('▁republikaner', 0.6422385573387146)]\n",
      "[('▁matt', 0.6543236374855042), ('▁griff', 0.6426835060119629), ('undet', 0.6052414178848267)]\n",
      "[('▁matt', 0.6375241875648499), ('▁vorsprung', 0.6192678213119507), ('▁griff', 0.6146632432937622)]\n",
      "[('▁calder', 0.6027392745018005), ('▁nelson', 0.599494993686676), ('▁sull', 0.5962843894958496)]\n",
      "[('▁nelson', 0.5932034850120544), ('▁sull', 0.5895329117774963), ('▁gamb', 0.5844269394874573)]\n",
      "[('lando', 0.5946722626686096), ('▁gamb', 0.5890830159187317), ('ingo', 0.5865989327430725)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    new_embed_add= embed_add\n",
    "    new_embed_add[0][i] = 1\n",
    "    print(bpemb_ger.most_similar(embed_add, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get alot of differend results, but are rather more distant to having \"kid\" in the top three than before. This is because the whole learnt language in the model had to be squashed into 25 dimensions. Thats far too less for each dimension to have a clear context. We just dont know, what each dimension represents.\n",
    "\n",
    "Lets try something else, restrict the information in the word vector more by adding extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁frau', 0.948681116104126),\n",
       " ('▁kind', 0.8942172527313232),\n",
       " ('▁mutter', 0.868741512298584),\n",
       " ('▁tod', 0.8545101284980774),\n",
       " ('▁mann', 0.8393333554267883),\n",
       " ('▁mannes', 0.8388955593109131)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Mann')\n",
    "embeds2 = bpemb_ger.embed('Frau')\n",
    "# Adding Birth\n",
    "embeds3 = bpemb_ger.embed('Geburt')\n",
    "\n",
    "embed_add = embeds1 + embeds2 + embeds3\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh look! We just got \"_kind\" (children) on the second place thorugh adding the word vector of \"birth\"\n",
    "That is pretty cool, right? But what happens, when we swap \"Mann\" and \"Frau\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁frau', 0.948681116104126),\n",
       " ('▁kind', 0.8942172527313232),\n",
       " ('▁mutter', 0.868741512298584),\n",
       " ('▁tod', 0.8545101284980774),\n",
       " ('▁mann', 0.8393333554267883),\n",
       " ('▁mannes', 0.8388955593109131)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Frau')\n",
    "embeds2 = bpemb_ger.embed('Mann')\n",
    "# Adding Birth\n",
    "embeds3 = bpemb_ger.embed('Geburt')\n",
    "\n",
    "embed_add = embeds1 + embeds2 + embeds3\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematicans wont be surprised, it is the same.\n",
    "Simply, because 1+2 is the same as 2+1. But, lets see if we can \"_kind\" even higher through weighting the word_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁frau', 0.9274512529373169),\n",
       " ('▁kind', 0.8905812501907349),\n",
       " ('▁geburt', 0.8828873038291931),\n",
       " ('▁mutter', 0.8547175526618958),\n",
       " ('▁gestorben', 0.8433364629745483),\n",
       " ('▁tod', 0.8418509364128113)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Frau')\n",
    "embeds2 = bpemb_ger.embed('Mann')\n",
    "embeds3 = bpemb_ger.embed('Geburt')\n",
    "\n",
    "# The information about birth is 1.5x more important than the information about man and woman\n",
    "embed_add = embeds1 + embeds2 + (1.5*embeds3)\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but we still did not got \"children\" to the top. Maybe Adding an extra information about children will raise it more. So lets puts Spielzeug (toys) in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁kind', 0.893251895904541),\n",
       " ('▁frau', 0.8610904216766357),\n",
       " ('▁ihm', 0.8160647749900818),\n",
       " ('▁ihr', 0.8110315799713135),\n",
       " ('▁mädchen', 0.8107687830924988),\n",
       " ('▁mutter', 0.8054994344711304)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Frau')\n",
    "embeds2 = bpemb_ger.embed('Mann')\n",
    "embeds3 = bpemb_ger.embed('Geburt')\n",
    "embeds4 = bpemb_ger.embed('Spielzeug')\n",
    "\n",
    "\n",
    "embed_add = embeds1 + embeds2 + (1.5*embeds3) + (2.0)*embeds4\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! Through adding the information \"Spielzeug\" we got the model to understand, that we talk about children. What do you think, can we change one word, to get the word \"baby\" to the top?\n",
    "\n",
    "Lets try changing \"Mann\" to Windeln (diapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁kind', 0.838711678981781),\n",
       " ('▁ihr', 0.7817391753196716),\n",
       " ('▁paar', 0.778824508190155),\n",
       " ('▁mädchen', 0.7774273157119751),\n",
       " ('▁ihrem', 0.7575104832649231),\n",
       " ('▁frau', 0.7430578470230103)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger.embed('Frau')\n",
    "embeds2 = bpemb_ger.embed('Windeln')\n",
    "embeds3 = bpemb_ger.embed('Geburt')\n",
    "embeds4 = bpemb_ger.embed('Spielzeug')\n",
    "\n",
    "\n",
    "embed_add = embeds1 + (2.0)*embeds2 + (1.5*embeds3) + (2.0)*embeds4\n",
    "bpemb_ger.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"kind\" is still on the top. Maybe the vocabulary is too small, to contain the word \"baby\", lets try it again with 50.000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10069301 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/de/de.wiki.bpe.vs100000.d25.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10069301/10069301 [00:02<00:00, 4629091.87B/s]\n"
     ]
    }
   ],
   "source": [
    "bpemb_ger_mid = BPEmb(lang=\"de\", vs=100000, dim=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁wunder', 0.7633698582649231),\n",
       " ('▁koffer', 0.7478492856025696),\n",
       " ('▁braut', 0.7169751524925232),\n",
       " ('▁glück', 0.714860737323761),\n",
       " ('▁liebes', 0.7134131789207458),\n",
       " ('▁baby', 0.7069087624549866)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds1 = bpemb_ger_mid.embed('Baby')\n",
    "embeds2 = bpemb_ger_mid.embed('Windeln')\n",
    "embeds3 = bpemb_ger_mid.embed('Geburt')\n",
    "embeds4 = bpemb_ger_mid.embed('Spielzeug')\n",
    "\n",
    "\n",
    "embed_add = embeds1 + (2.0)*embeds2 + (1.5*embeds3) + (2.0)*embeds4\n",
    "bpemb_ger_mid.most_similar(embed_add, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, isn't it? Still we don't get \"Baby\" to the top. Lets see, what is similar to baby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁baby', 1.0),\n",
       " ('baby', 0.8973329663276672),\n",
       " ('good', 0.877951979637146),\n",
       " ('▁happy', 0.8630719780921936),\n",
       " ('▁love', 0.8501371741294861),\n",
       " ('love', 0.8397102355957031),\n",
       " ('▁like', 0.8358559012413025),\n",
       " ('▁christmas', 0.8313241004943848),\n",
       " ('▁girls', 0.8312001824378967),\n",
       " ('▁sweet', 0.8268948793411255)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_baby = bpemb_ger_mid.embed('Baby')\n",
    "bpemb_ger_mid.most_similar(embeds_baby, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the word baby has different meanings. In the german bpemb model, there are alot of english words to find, thats where \"baby\" can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post you learned how to use the bpemb model, to extract word embeddings to analyze them. You also learned how to find other similar words and how bpemb stores them in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_nightly_env",
   "language": "python",
   "name": "tf_nightly_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
